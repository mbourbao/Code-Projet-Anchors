{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import string \n",
    "from nltk import word_tokenize\n",
    "import os\n",
    "os.chdir(\"C:/Users/33651/Documents/Projet_Anchors/CODE/\")\n",
    "from selection_meilleur_ancre_DETERMINISTE import * \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Chargement des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy 0.8\n"
     ]
    }
   ],
   "source": [
    "from chargement_donnée import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy 0.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "c = sklearn.linear_model.LogisticRegression()\n",
    "# c = sklearn.ensemble.RandomForestClassifier(n_estimators=500, n_jobs=10)\n",
    "c.fit(train_vectors, train_labels)\n",
    "preds = c.predict(val_vectors)\n",
    "print('Val accuracy', sklearn.metrics.accuracy_score(val_labels, preds))\n",
    "def predict_lr(texts):\n",
    "    return c.predict(vectorizer.transform(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=True)\n",
    "\n",
    "text = \"The reception has been generally good\"\n",
    "pred = explainer.class_names[predict_lr([text])[0]]\n",
    "print('Prediction: %s' % pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La phrase : The reception has been generally good reflète un sentiment positif\n"
     ]
    }
   ],
   "source": [
    "label = predict_lr([text])[0]\n",
    "\n",
    "if label == 1 : \n",
    "    print(\"La phrase : The reception has been generally good reflète un sentiment positif\" )\n",
    "else : \n",
    "     print(\"La phrase : The reception has been generally good reflète un sentiment négatif\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour la phrase : The reception has been generally good\n",
      "L'ancre donnée par mon implémentation de Anchors est : ['good']\n",
      "L'ancre donnée par le package est :  ['good']\n",
      "La précision obtenue avec mon implémentation est 1.0\n",
      "La précision obtenue avec le package est  1.0\n"
     ]
    }
   ],
   "source": [
    "tau = 0.15\n",
    "#Application de la nouvelle implémentation.\n",
    "mon_ancre = meilleur_ancre_bis_new(df,text,500,\"UNK\",label,tau,vectorizer, c)\n",
    "#Récupération du nom de l'ancre obtenue.\n",
    "nom_ancre = mon_ancre[0]\n",
    "#Récupération de la précision associée à l'ancre obtenue. \n",
    "precision_ancre = mon_ancre[1]\n",
    "\n",
    "#Application du package Anchors de python\n",
    "####################################################\n",
    "ancre_package = explainer.explain_instance(text, predict_lr, threshold=0.95)\n",
    "ancre_p_nom = ancre_package.names()\n",
    "ancre_p_precision = ancre_package.precision()\n",
    "\n",
    "\n",
    "#####################################################\n",
    "#Affichage \n",
    "\n",
    "print(\"Pour la phrase : The reception has been generally good\" ) \n",
    "print()\n",
    "print(\"L'ancre donnée par mon implémentation de Anchors est :\", nom_ancre)\n",
    "print(\"L'ancre donnée par le package est : \", ancre_p_nom)\n",
    "print()\n",
    "print(\"La précision obtenue avec mon implémentation est\", precision_ancre )\n",
    "print(\"La précision obtenue avec le package est \", ancre_p_precision )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La phrase \"The reception has been generally good\" a été classer par la boîte noire dans les commentaires positifs. Le problème est qu'on ne sait pas sur quoi le classifier s'est basé pour prendre cette descision. \n",
    "Anchors nous explique que c'est grâce au mot \"good\" présent dans la phrase que le classifier a classer l'instance dans les positifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
